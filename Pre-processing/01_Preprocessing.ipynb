{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMC data processing pipeline**\n",
    " \n",
    "This pipeline has been specifically developed for the Imaging Mass Cytometry (IMC) - Type 1 Diabetes (T1D) project.\n",
    " \n",
    "## **Introduction**\n",
    " \n",
    "This pipeline extracts image data from Imaging Mass Cytometry aquisitions, performs islet- and cell-level image segmentation and extracts measurements from the segmented objects.  \n",
    "This pipeline is designed to work with two antibody panels applied to two consecutive tissue sections.\n",
    "\n",
    "As input, the user should provide zipped folders containing IMC acquisition (one `.mcd` file with the associated `.txt` files), and a panel file (`panel.csv`) for each antibody panel that indicates the channels that were measured and the channels that should be used for segmentation. Detailed information about zipped folders and panel files can be found below.\n",
    " \n",
    "This pipeline is based on functions from the [steinbock package](https://github.com/BodenmillerGroup/steinbock), full steinbock documentation can be found here: https://bodenmillergroup.github.io/steinbock.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Preprocessing**\n",
    " \n",
    "## **Configuration**\n",
    " \n",
    "### **Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/T1D_preprocessing', '/opt/conda/lib/python39.zip', '/opt/conda/lib/python3.9', '/opt/conda/lib/python3.9/lib-dynload', '', '/opt/conda/lib/python3.9/site-packages']\n",
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from steinbock import io\n",
    "from steinbock.preprocessing import imc\n",
    "print(sys.path)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to get unique elements from 2 lists with elements of list 1 having .tiff suffixes.\n",
    "def get_unique_elements(list1, list2):\n",
    "    # Include some \" - split here\"\n",
    "    return [element for element in list1 if not any(element.replace(\" - split.tiff\", \"\").replace(\".tiff\", \"\") in item for item in list2)]\n",
    "\n",
    "# Helper function to get all duplicates in 2 lists.\n",
    "def get_duplicates(lst):\n",
    "    unique_elements = set()\n",
    "    duplicates = []\n",
    "    for item in lst:\n",
    "        if item in unique_elements:\n",
    "            duplicates.append(item)\n",
    "        else:\n",
    "            unique_elements.add(item)\n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define input and output directories**\n",
    " \n",
    "*Manual step:* enter the path to the directory where the data will be saved (named `folder_data` from here on).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder: /home/processing\n",
      "Git folder: /home/T1D_preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Data folder\n",
    "folder_data = Path(\"/home/processing/\")\n",
    "Path(folder_data).mkdir(parents=True, exist_ok=True)\n",
    "assert Path.exists(folder_data), f\"{folder_data} does not exist\"\n",
    "print(\"Data folder:\", folder_data)\n",
    "\n",
    "# Git folder (folder containing the current notebook)\n",
    "folder_git = Path.cwd()\n",
    "assert Path.exists(folder_git), f\"{folder_git} does not exist\"\n",
    "print(\"Git folder:\", folder_git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create folders for intermediate processing steps**\n",
    "- `raw`: should contain user-provided zipped `.mcd` and `.txt` acquisitions.\n",
    "- `img`: store extracted images in `.tiff` format.\n",
    "- `seg_cells`: store image stacks for cell segmentation.\n",
    "- `masks_cells`: store cell segmentation masks.\n",
    "- `data_cells`: store generated single cell-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\n",
    "    \"raw\": folder_data / \"raw\",\n",
    "    \"img\": folder_data / \"img\",\n",
    "    \"seg_cells\": folder_data / \"seg_cells\",\n",
    "    \"masks_cells\": folder_data / \"masks_cells\",\n",
    "    \"data_cells\": folder_data / \"data_cells\",\n",
    "    \"variables\": folder_data / \"variables\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories (if they do not exist)\n",
    "for folder in folders.values():\n",
    "    folder.mkdir(exist_ok=True)\n",
    "    \n",
    "# Add base previously defined data and git folders\n",
    "folders[\"data\"] = folder_data\n",
    "folders[\"git\"] = folder_git\n",
    "\n",
    "# Export folder names for use in downstream notebooks\n",
    "with open(folder_data / \"variables\" / \"folders.txt\", \"wb\") as handle:\n",
    "    pickle.dump(folders, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Antibody panels**\n",
    " \n",
    "Panel files are user-provided `.csv` files that should be located in `folder_data` and contain the following columns:\n",
    "- `channel`: unique channel ID.\n",
    "- `metal`: metal isotope to which the antibody is conjugated (format: `Nd144`, `Ir191`).\n",
    "- `name`: name of the target marker.\n",
    "- `keep` should the channel be retained for processing and analysis? (1 = yes, 0 = no).\n",
    "- `deepcell` should the channel be used for cell segmentation and in which compartment the marker is expressed? (1 = nucleus, 2 = membrane, empty if the channel should not be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns required in the panel file(s)\n",
    "panel_cols = {\n",
    "    \"col_channel\": \"channel\",\n",
    "    \"col_metal\": \"metal\",\n",
    "    \"col_name\": \"name\",\n",
    "    \"col_keep\": \"keep\",\n",
    "    \"col_deeepcell\": \"deepcell\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Manual step:* adapt the panel names and panel file names if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List panel files\n",
    "panels = {\n",
    "    \"Uncompressed\": (folder_data / 'panel_Uncompressed.csv'),\n",
    "    \"Compressed\": (folder_data / 'panel_Compressed.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load and display the panels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panel: Uncompressed\n",
      "   channel  metal                     name    antibody_clone  keep  deepcell  \\\n",
      "0        0  In113               Histone H3              D1H2     1       1.0   \n",
      "1        1  In115                      SMA               1A4     1       2.0   \n",
      "5        5  Pr141                  insulin             C27C9     1       0.0   \n",
      "6        6  Nd143                     CD44               IM7     1       2.0   \n",
      "7        7  Nd144  Prohormone Convertase 2  Polyclonal _ PC2     1       0.0   \n",
      "\n",
      "   clustering  dimred shortname  \n",
      "0           0       0        H3  \n",
      "1           1       1       SMA  \n",
      "5           1       1       INS  \n",
      "6           1       1      CD44  \n",
      "7           1       1     PCSK2  \n",
      "Panel: Compressed\n",
      "   channel  metal                     name  antibody_clone  keep  deepcell  \\\n",
      "0        0  In113               Histone H3            D1H2     1       1.0   \n",
      "1        1  In115                      SMA             1A4     1       2.0   \n",
      "5        5  Nd143                 CD44_GCG      IM7_D16G10     1       2.0   \n",
      "6        6  Nd144  Prohormone Convertase 2  Polyclonal_PC2     1       0.0   \n",
      "7        7  Nd145                     CD99           HCD99     1       2.0   \n",
      "\n",
      "   clustering  dimred  categories shortname  \n",
      "0           0       0           0        H3  \n",
      "1           1       1           1       SMA  \n",
      "5           1       1           0  CD44_GCG  \n",
      "6           1       1           0     PCSK2  \n",
      "7           1       1           0      CD99  \n"
     ]
    }
   ],
   "source": [
    "# Loop through the panels\n",
    "for panel_name, panel_path in panels.items():\n",
    "    print(\"Panel:\", panel_name)\n",
    "    \n",
    "    # Load the panel file\n",
    "    assert Path.exists(panel_path), f\"{panel_path} does not exist\"\n",
    "    cur_panel = pd.read_csv(panel_path, sep = ',', index_col = False)\n",
    "\n",
    "    # Make sure that the required columns exist\n",
    "    for col in panel_cols.values():\n",
    "        assert(col in cur_panel.columns), f\"Column {col} missing from panel\"\n",
    "    \n",
    "    # Subset the panel\n",
    "    cur_panel = cur_panel[cur_panel[panel_cols[\"col_keep\"]]==1]\n",
    "    panels[panel_name] = cur_panel\n",
    "    \n",
    "    # Display the panel\n",
    "    print(panels[panel_name].head())\n",
    "    \n",
    "# Export the panels for use in downstream scripts\n",
    "with open(folder_data / \"variables\" / \"panels.txt\", \"wb\") as handle:\n",
    "     pickle.dump(panels, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Process zipped folders**\n",
    " \n",
    "IMC acquisitions generate `.mcd` and `.txt` files. Each acquisition session (corresponding ot one `.mcd` file) should be zipped in a folder containing:\n",
    "- The `.mcd` file.\n",
    "- All the associated `.txt` files generated during the acquisition (do not change any of the file names).\n",
    "\n",
    "The `.txt` files are used as a backup in case the data cannot be extracted from the `.mcd` file. \n",
    "All the zipped folders should be stored in subfolders of the `raw` folder (in the `folder_data` directory). The subfolders should be named exactly like the panels in `panels` (see \"List panel files\" above).   \n",
    "\n",
    "For the current dataset, the folder structure is the following, with zipped MCD and TXT files stored in `raw/Immune` and `raw/Islet`:\n",
    "\n",
    "folder_data\n",
    "|_ data_cells\n",
    "|_ data_islets\n",
    "|_ img\n",
    "|_ masks_cells\n",
    "|_ masks_islets\n",
    "|_ raw\n",
    "    |_ Immune <- ZIP files from the Immune panel stored here\n",
    "    |_ Islet  <- ZIP files from the Islet panel stored here\n",
    "|_ seg_cells\n",
    "|_ seg_islets\n",
    "|\n",
    "|_ panel_Immune.csv <- Panel file (Immune panel)\n",
    "|_ panel_Islet.csv  <- Panel file (Islet panel)\n",
    "\n",
    "### **List `.zip` folders**\n",
    "*Manual step:* define a regular expression to identify the naming scheme of `.zip` files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part that all zipped files need to have in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of '.zip' folders: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Compressed</th>\n",
       "      <th>Uncompressed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>6227_Compressed.mcd</td>\n",
       "      <td>6227_Uncompressed.mcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6396</th>\n",
       "      <td>6396_Compressed.mcd</td>\n",
       "      <td>6396_Uncompressed.mcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>6238_Compressed.mcd</td>\n",
       "      <td>6238_Uncompressed.mcd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6399</th>\n",
       "      <td>6399_Compressed.mcd</td>\n",
       "      <td>6399_Uncompressed.mcd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Compressed           Uncompressed\n",
       "6227  6227_Compressed.mcd  6227_Uncompressed.mcd\n",
       "6396  6396_Compressed.mcd  6396_Uncompressed.mcd\n",
       "6238  6238_Compressed.mcd  6238_Uncompressed.mcd\n",
       "6399  6399_Compressed.mcd  6399_Uncompressed.mcd"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_regex = '(?P<caseid>[0-9]{3,4})_(?P<panel>[a-zA-Z0-9]+)*'\n",
    "file_regex = '(?P<caseid>[0-9]{3,4})_(?P<panel>[a-zA-Z0-9]+)'\n",
    "\n",
    "#List all zip folders that match the regular expression.\n",
    "\n",
    "# List zip folders. This should 190 folders.\n",
    "re_fn = re.compile(file_regex)\n",
    "zip_folders = [f for f in folders['raw'].rglob(\"*\") if \n",
    "               re.match(file_regex, f.name)]\n",
    "print(\"\\nTotal number of '.zip' folders:\", len(zip_folders))\n",
    "\n",
    "# List all case IDs and panels\n",
    "case_list = []\n",
    "panel_list = []\n",
    "\n",
    "for file in zip_folders:\n",
    "    case_list.append(re_fn.search(file.name).group(\"caseid\"))\n",
    "    panel_list.append(re_fn.search(file.name).group(\"panel\"))\n",
    "\n",
    "case_list = set(case_list)\n",
    "panel_list = set(panel_list)\n",
    "\n",
    "# Generate a table with case IDs as indexes and panels as columns\n",
    "zip_table = pd.DataFrame(dtype=str, columns=panel_list, index=case_list)\n",
    "\n",
    "for file in zip_folders:\n",
    "    cur_case = re_fn.search(file.name).group(\"caseid\")\n",
    "    cur_panel = re_fn.search(file.name).group(\"panel\")\n",
    "    zip_table.loc[cur_case, cur_panel] = file.name\n",
    "zip_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract images from IMC acquisitions**\n",
    " \n",
    "Here, images are extracted from raw IMC files and saved in the `img` folder. Each image corresponds to one acquisition in one file, with the image channels filtered (`keep` column in antibody panel) and sorted according to the the panel file.  \n",
    " \n",
    "In case an `.mcd` file is corrupted, the steinbock function tries to extract missing acquisitions from matching `.txt` files. In a second step, images from unmatched `.txt` files are extracted as well.  \n",
    "\n",
    "See the full documentation here: https://bodenmillergroup.github.io/steinbock/latest/cli/preprocessing/#image-conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Settings**\n",
    "After image extraction, hot pixel filtering is performed using the threshold defined by the `hot_pixel_filtering` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_pixel_filtering = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here: fix potential mismatched regions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Image conversion**\n",
    "Extract image stacks from IMC acquisitions. \n",
    "Image and acquisition metadata are exported to `folder_data` as `images.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Uncompressed panel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error reading acquisition 33 from file /home/processing/raw/Uncompressed/6399_Uncompressed/6399_Uncompressed.mcd: MCD file '6399_Uncompressed.mcd' corrupted: inconsistent acquisition image data size\n",
      "Error reading acquisition 35 from file /home/processing/raw/Uncompressed/6399_Uncompressed/6399_Uncompressed.mcd: MCD file '6399_Uncompressed.mcd' corrupted: inconsistent acquisition image data size\n",
      "Error reading acquisition 2 from file /home/processing/raw/Uncompressed/6238_Uncompressed/6238_Uncompressed.mcd: MCD file '6238_Uncompressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 5 from file /home/processing/raw/Uncompressed/6227_Uncompressed/6227_Uncompressed.mcd: MCD file '6227_Uncompressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 23 from file /home/processing/raw/Uncompressed/6227_Uncompressed/6227_Uncompressed.mcd: MCD file '6227_Uncompressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 26 from file /home/processing/raw/Uncompressed/6227_Uncompressed/6227_Uncompressed.mcd: MCD file '6227_Uncompressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 29 from file /home/processing/raw/Uncompressed/6227_Uncompressed/6227_Uncompressed.mcd: MCD file '6227_Uncompressed.mcd' corrupted: invalid acquisition image data offsets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Compressed panel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error reading acquisition 19 from file /home/processing/raw/Compressed/6396_Compressed/6396_Compressed.mcd: MCD file '6396_Compressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 27 from file /home/processing/raw/Compressed/6227_Compressed/6227_Compressed.mcd: MCD file '6227_Compressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 29 from file /home/processing/raw/Compressed/6227_Compressed/6227_Compressed.mcd: MCD file '6227_Compressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 30 from file /home/processing/raw/Compressed/6227_Compressed/6227_Compressed.mcd: MCD file '6227_Compressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 32 from file /home/processing/raw/Compressed/6227_Compressed/6227_Compressed.mcd: MCD file '6227_Compressed.mcd' corrupted: invalid acquisition image data offsets\n",
      "Error reading acquisition 33 from file /home/processing/raw/Compressed/6227_Compressed/6227_Compressed.mcd: MCD file '6227_Compressed.mcd' corrupted: invalid acquisition image data offsets\n"
     ]
    }
   ],
   "source": [
    "panels[\"Uncompressed\"]\n",
    "\n",
    "for panel_name, panel in panels.items():\n",
    "    print(\"Processing\", panel_name, \"panel\")\n",
    "    \n",
    "    # Input and output folders\n",
    "    image_info = []\n",
    "    raw_subdir = folders[\"raw\"] / panel_name\n",
    "    img_subdir = folders[\"img\"] / panel_name\n",
    "    img_subdir.mkdir(exist_ok = True)  \n",
    "    \n",
    "    # List zipped files\n",
    "    cur_mcd_files = imc.list_mcd_files(raw_subdir, unzip=True)\n",
    "    cur_txt_files = imc.list_txt_files(raw_subdir, unzip=True)\n",
    "    \n",
    "    # Process files\n",
    "    for (mcd_file, acquisition, img, matched_txt, recovered) in \\\n",
    "    imc.try_preprocess_images_from_disk(\n",
    "        cur_mcd_files, cur_txt_files,\n",
    "        hpf = hot_pixel_filtering,\n",
    "        channel_names = panels[panel_name][\"metal\"],\n",
    "        unzip = True\n",
    "    ):\n",
    "        cur_desc = acquisition.description\n",
    "        cur_case = re_fn.search(mcd_file.name).group(\"caseid\")\n",
    "        \n",
    "        img_file = f\"{mcd_file.stem}_{cur_desc}.tiff\"\n",
    "        io.write_image(img, img_subdir / img_file)\n",
    "\n",
    "        # Save acquisition metadata\n",
    "        image_info_row = imc.create_image_info(\n",
    "            mcd_file, acquisition, img, matched_txt, recovered, img_file\n",
    "        )\n",
    "    \n",
    "        image_info_row[\"panel\"] = panel_name\n",
    "        image_info.append(image_info_row)\n",
    "\n",
    "    image_info = pd.DataFrame(image_info)\n",
    "    image_meta_file = f\"images_{panel_name}.csv\"\n",
    "    image_info.to_csv(folders[\"data\"] / image_meta_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Catch unmatched images** \n",
    "\n",
    "### **Flag missing images**\n",
    "The ablated regions should be the same on all consecutive sections. Here, we attempt to match images from different panels, based on the ROI number. Images from one panel that do not have a corresponding image in the other panel(s) are flagged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with missing corresponding images ( 9 missing images ):\n",
      " ['6227__ROI_005.tiff', '6227__ROI_025.tiff', '6227__ROI_028.tiff', '6227__ROI_035_test.tiff', '6238__ROI_002.tiff', '6238__ROI_031.tiff', '6396__ROI_019.tiff', '6399__Test-2.tiff', '6399__Test-4.tiff']\n"
     ]
    }
   ],
   "source": [
    "panel_names = list(panels.keys())\n",
    "missing = set()\n",
    "\n",
    "# List files for the first panel\n",
    "images_panel0 = sorted([img.name.replace(panel_names[0], \"\") \\\n",
    "                        for img in Path.iterdir(folders[\"img\"] / panel_names[0])])\n",
    "images_panel0 = frozenset(images_panel0)\n",
    "\n",
    "# Find matched images in the other panels\n",
    "for panel_name in panel_names[1:]:\n",
    "    cur_images = [img.name for img in Path.iterdir(folders[\"img\"] / panel_name)]\n",
    "    cur_list = set([img.replace(panel_name, \"\") for \\\n",
    "                    img in cur_images])\n",
    "    \n",
    "    missing.add(frozenset(images_panel0.difference(cur_list)))\n",
    "    missing.add(frozenset(cur_list.difference(images_panel0)))\n",
    "\n",
    "# Print out all missing images\n",
    "missing = [list(x) for x in missing]\n",
    "missing = sorted([x for xs in missing for x in xs])\n",
    "print(\"Images with missing corresponding images (\", len(missing),\n",
    "      \"missing images ):\\n\", missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIXME: If there is a problem with next part: remember here we adjusted script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Delete unmatched images**\n",
    "\n",
    "Images that do not have a matching image in all the other panels are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting /home/processing/img/Uncompressed/6227_Uncompressed_ROI_005.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6227_Uncompressed_ROI_025.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6227_Uncompressed_ROI_028.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6227_Uncompressed_ROI_035_test.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6238_Uncompressed_ROI_002.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6238_Uncompressed_ROI_031.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6396_Uncompressed_ROI_019.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6399_Uncompressed_Test-2.tiff\n",
      "Deleting /home/processing/img/Uncompressed/6399_Uncompressed_Test-4.tiff\n",
      "Deleting /home/processing/img/Compressed/6227_Compressed_ROI_005.tiff\n",
      "Deleting /home/processing/img/Compressed/6227_Compressed_ROI_025.tiff\n",
      "Deleting /home/processing/img/Compressed/6227_Compressed_ROI_028.tiff\n",
      "Deleting /home/processing/img/Compressed/6227_Compressed_ROI_035_test.tiff\n",
      "Deleting /home/processing/img/Compressed/6238_Compressed_ROI_002.tiff\n",
      "Deleting /home/processing/img/Compressed/6238_Compressed_ROI_031.tiff\n",
      "Deleting /home/processing/img/Compressed/6396_Compressed_ROI_019.tiff\n",
      "Deleting /home/processing/img/Compressed/6399_Compressed_Test-2.tiff\n",
      "Deleting /home/processing/img/Compressed/6399_Compressed_Test-4.tiff\n"
     ]
    }
   ],
   "source": [
    "delete_unmatched_images = True\n",
    "\n",
    "if missing and delete_unmatched_images:\n",
    "    for panel_name in panel_names:\n",
    "        cur_dir = folders[\"img\"] / panel_name\n",
    "        unmatched_images = [\n",
    "            cur_dir / im.replace(\"__\", (\"_\" + panel_name + \"_\")) \\\n",
    "            for im in missing]\n",
    "        \n",
    "        for image in unmatched_images:\n",
    "            print(f\"Deleting {image}\")\n",
    "            Path.unlink(image, missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next steps**\n",
    "\n",
    "The next step in this pipeline is cell segmentation, which is performed with the `02_CellSegmentation.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
